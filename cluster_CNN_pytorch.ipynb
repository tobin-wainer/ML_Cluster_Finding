{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from astropy.io import fits\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "K_FOLDS = 5\n",
    "\n",
    "IMG_SIZE = (301, 301)         # height, width of images\n",
    "IN_CHANNELS = 2               # two bands\n",
    "N_LAYERS = 3                  # number of conv layers\n",
    "CONV_CHANNELS = 32             # channels in conv layers\n",
    "KERNEL_SIZE = 3\n",
    "DROPOUT = 0.2\n",
    "BATCH_NORM = True\n",
    "\n",
    "DATA_DIR = \"practice_data\"\n",
    "LABELS_CSV = \"channel_prob_data.csv\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FITSDataset(Dataset):\n",
    "    def __init__(self, csv_file, data_dir, target_size=(301,301)):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.data_dir = data_dir\n",
    "        self.target_size = target_size\n",
    "\n",
    "        # Optional: filter rows to only files that exist\n",
    "        def files_exist(row):\n",
    "            file1 = os.path.join(data_dir, row['f475_image_string'])\n",
    "            file2 = os.path.join(data_dir, row['f814_image_string'])\n",
    "            return os.path.isfile(file1) and os.path.isfile(file2)\n",
    "        self.df = self.df[self.df.apply(files_exist, axis=1)].reset_index(drop=True)\n",
    "        print(f\"Filtered dataset length: {len(self.df)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        band1 = self.load_fits(os.path.join(self.data_dir, row['f475_image_string']))\n",
    "        band2 = self.load_fits(os.path.join(self.data_dir, row['f814_image_string']))\n",
    "\n",
    "        stacked = np.stack([band1, band2], axis=0)\n",
    "        label = np.float32(row['prob'])\n",
    "\n",
    "        return torch.tensor(stacked, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "    def load_fits(self, filepath):\n",
    "        with fits.open(filepath) as hdul:\n",
    "            data = hdul[0].data.astype(np.float32)\n",
    "\n",
    "        data = np.squeeze(data)\n",
    "\n",
    "        # Replace NaNs with median\n",
    "        median_val = np.nanmedian(data)\n",
    "        data = np.nan_to_num(data, nan=median_val)\n",
    "\n",
    "        # Center crop or pad to target_size\n",
    "        H, W = data.shape\n",
    "        target_H, target_W = self.target_size\n",
    "\n",
    "        # Crop if bigger than target\n",
    "        start_H = max((H - target_H) // 2, 0)\n",
    "        start_W = max((W - target_W) // 2, 0)\n",
    "        cropped = data[start_H:start_H+target_H, start_W:start_W+target_W]\n",
    "\n",
    "        # Pad if smaller than target\n",
    "        pad_H = max(target_H - cropped.shape[0], 0)\n",
    "        pad_W = max(target_W - cropped.shape[1], 0)\n",
    "        if pad_H > 0 or pad_W > 0:\n",
    "            cropped = np.pad(cropped, ((0, pad_H), (0, pad_W)), mode='constant', constant_values=median_val)\n",
    "\n",
    "        return cropped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset length: 20\n",
      "Number of image pairs: 20\n",
      "\n",
      "Image 0: shape=torch.Size([2, 301, 301]), label=0.0\n",
      "Min/Max values: 0.34768664836883545, 21.175325393676758\n",
      "\n",
      "Image 1: shape=torch.Size([2, 301, 301]), label=0.0\n",
      "Min/Max values: 0.3239341974258423, 21.22384262084961\n",
      "\n",
      "Image 2: shape=torch.Size([2, 301, 301]), label=0.0\n",
      "Min/Max values: 0.2799783945083618, 18.657468795776367\n",
      "\n",
      "Image 3: shape=torch.Size([2, 301, 301]), label=0.0\n",
      "Min/Max values: 0.39322853088378906, 18.384241104125977\n",
      "\n",
      "Image 4: shape=torch.Size([2, 301, 301]), label=0.0\n",
      "Min/Max values: 0.3829287886619568, 18.384241104125977\n",
      "\n",
      "Image 5: shape=torch.Size([2, 301, 301]), label=0.0\n",
      "Min/Max values: 0.362567275762558, 20.14598846435547\n",
      "\n",
      "Image 6: shape=torch.Size([2, 301, 301]), label=0.0\n",
      "Min/Max values: 0.34225305914878845, 17.620946884155273\n",
      "\n",
      "Image 7: shape=torch.Size([2, 301, 301]), label=0.0\n",
      "Min/Max values: 0.2941800653934479, 17.429018020629883\n",
      "\n",
      "Image 8: shape=torch.Size([2, 301, 301]), label=0.0\n",
      "Min/Max values: 0.2941800653934479, 17.18610954284668\n",
      "\n",
      "Image 9: shape=torch.Size([2, 301, 301]), label=0.0\n",
      "Min/Max values: 0.40539073944091797, 20.016738891601562\n",
      "\n",
      "Image 10: shape=torch.Size([2, 301, 301]), label=0.7900000214576721\n",
      "Min/Max values: 0.5236894488334656, 25.502418518066406\n",
      "\n",
      "Image 11: shape=torch.Size([2, 301, 301]), label=0.7599999904632568\n",
      "Min/Max values: 0.21682871878147125, 150.0795135498047\n",
      "\n",
      "Image 12: shape=torch.Size([2, 301, 301]), label=0.7599999904632568\n",
      "Min/Max values: 0.20634689927101135, 230.55030822753906\n",
      "\n",
      "Image 13: shape=torch.Size([2, 301, 301]), label=0.6100000143051147\n",
      "Min/Max values: 0.1765369176864624, 35.40043640136719\n",
      "\n",
      "Image 14: shape=torch.Size([2, 301, 301]), label=0.6100000143051147\n",
      "Min/Max values: 0.16044890880584717, 35.40043640136719\n",
      "\n",
      "Image 15: shape=torch.Size([2, 301, 301]), label=0.6100000143051147\n",
      "Min/Max values: 0.0, 35.40043640136719\n",
      "\n",
      "Image 16: shape=torch.Size([2, 301, 301]), label=0.6100000143051147\n",
      "Min/Max values: 0.0, 35.40043640136719\n",
      "\n",
      "Image 17: shape=torch.Size([2, 301, 301]), label=0.6800000071525574\n",
      "Min/Max values: 0.7068445682525635, 24.102489471435547\n",
      "\n",
      "Image 18: shape=torch.Size([2, 301, 301]), label=0.8600000143051147\n",
      "Min/Max values: 0.39741113781929016, 28.592233657836914\n",
      "\n",
      "Image 19: shape=torch.Size([2, 301, 301]), label=0.8999999761581421\n",
      "Min/Max values: 0.8658038973808289, 40.63483428955078\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Initialize dataset\n",
    "test_dataset = FITSDataset(\"channel_prob_data.csv\", \"practice_data/\")\n",
    "\n",
    "print(f\"Number of image pairs: {len(test_dataset)}\\n\")\n",
    "\n",
    "# Check each image\n",
    "for i in range(len(test_dataset)):\n",
    "    image_tensor, label = test_dataset[i]\n",
    "    print(f\"Image {i}: shape={image_tensor.shape}, label={label}\")\n",
    "    print(f\"Min/Max values: {image_tensor.min()}, {image_tensor.max()}\\n\")\n",
    "\n",
    "# Optional: visualize a few random images to check cropping\n",
    "for i in range(min(0, len(test_dataset))):\n",
    "    image_tensor, label = test_dataset[i]\n",
    "    band1 = image_tensor[0].numpy()\n",
    "    band2 = image_tensor[1].numpy()\n",
    "\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(band1, cmap='gray')\n",
    "    plt.title(\"Band 1\")\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(band2, cmap='gray')\n",
    "    plt.title(\"Band 2\")\n",
    "    plt.suptitle(f\"Label: {label}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlexibleCNN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels=IN_CHANNELS,      # use hyperparameter\n",
    "                 n_layers=N_LAYERS,           # use hyperparameter\n",
    "                 conv_channels=CONV_CHANNELS, # use hyperparameter\n",
    "                 kernel_size=KERNEL_SIZE,     # use hyperparameter\n",
    "                 img_size=IMG_SIZE,           # use hyperparameter\n",
    "                 dropout=DROPOUT,             # use hyperparameter\n",
    "                 batch_norm=BATCH_NORM):      # use hyperparameter\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        # First conv layer\n",
    "        layers.append(nn.Conv2d(in_channels, conv_channels, kernel_size=kernel_size, padding=kernel_size//2))\n",
    "        layers.append(nn.BatchNorm2d(conv_channels) if batch_norm else nn.Identity())\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.MaxPool2d(2))\n",
    "        layers.append(nn.Dropout2d(dropout) if dropout > 0 else nn.Identity())\n",
    "\n",
    "        # Additional conv layers\n",
    "        for i in range(1, n_layers):\n",
    "            layers.append(nn.Conv2d(conv_channels, conv_channels, kernel_size=kernel_size, padding=kernel_size//2))\n",
    "            layers.append(nn.BatchNorm2d(conv_channels) if batch_norm else nn.Identity())\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.MaxPool2d(2))\n",
    "            layers.append(nn.Dropout2d(dropout) if dropout > 0 else nn.Identity())\n",
    "\n",
    "        self.conv_model = nn.Sequential(*layers)\n",
    "\n",
    "        # Flattened size\n",
    "        H, W = img_size\n",
    "        H //= 2 ** n_layers\n",
    "        W //= 2 ** n_layers\n",
    "        flattened_size = conv_channels * H * W\n",
    "\n",
    "        # Fully connected output\n",
    "        self.fc_model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(flattened_size, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Loss function\n",
    "        self.loss = nn.BCELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_model(x)\n",
    "        x = self.fc_model(x)\n",
    "        return x.squeeze(1)\n",
    "\n",
    "    def configure_optimizers(self, learning_rate=LR, weight_decay=WEIGHT_DECAY):  # use hyperparameter\n",
    "        return optim.Adam(self.parameters(), lr=learning_rate, weight_decay=weight_decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset length: 20\n",
      "\n",
      "Fold 1/5\n",
      "Epoch 1, Train Loss: 0.7383, Val Loss: 0.6381\n",
      "Saved best model for fold 1\n",
      "Epoch 2, Train Loss: 2.2347, Val Loss: 1.2712\n",
      "Epoch 3, Train Loss: 2.9418, Val Loss: 1.1307\n",
      "Epoch 4, Train Loss: 1.9222, Val Loss: 0.8076\n",
      "Epoch 5, Train Loss: 1.0049, Val Loss: 0.7877\n",
      "Fold 1 Metrics:\n",
      "MSE: 0.1817, MAE: 0.3769, R²: -0.7372\n",
      "\n",
      "Fold 2/5\n",
      "Epoch 1, Train Loss: 0.6701, Val Loss: 0.6129\n",
      "Saved best model for fold 2\n",
      "Epoch 2, Train Loss: 5.3028, Val Loss: 2.3291\n",
      "Epoch 3, Train Loss: 1.6007, Val Loss: 4.0271\n",
      "Epoch 4, Train Loss: 7.2496, Val Loss: 4.7108\n",
      "Epoch 5, Train Loss: 1.6497, Val Loss: 4.6772\n",
      "Fold 2 Metrics:\n",
      "MSE: 0.7563, MAE: 0.7995, R²: -5.9829\n",
      "\n",
      "Fold 3/5\n",
      "Epoch 1, Train Loss: 0.7773, Val Loss: 1.1757\n",
      "Saved best model for fold 3\n",
      "Epoch 2, Train Loss: 2.8234, Val Loss: 0.7033\n",
      "Saved best model for fold 3\n",
      "Epoch 3, Train Loss: 0.5359, Val Loss: 0.9169\n",
      "Epoch 4, Train Loss: 2.3977, Val Loss: 1.1196\n",
      "Epoch 5, Train Loss: 2.8926, Val Loss: 1.0576\n",
      "Fold 3 Metrics:\n",
      "MSE: 0.2154, MAE: 0.3241, R²: -1.1426\n",
      "\n",
      "Fold 4/5\n",
      "Epoch 1, Train Loss: 0.7482, Val Loss: 2.8867\n",
      "Saved best model for fold 4\n",
      "Epoch 2, Train Loss: 9.6475, Val Loss: 4.7989\n",
      "Epoch 3, Train Loss: 10.6182, Val Loss: 6.6618\n",
      "Epoch 4, Train Loss: 7.3141, Val Loss: 8.4576\n",
      "Epoch 5, Train Loss: 6.3341, Val Loss: 9.4641\n",
      "Fold 4 Metrics:\n",
      "MSE: 0.3469, MAE: 0.4150, R²: -0.9860\n",
      "\n",
      "Fold 5/5\n",
      "Epoch 1, Train Loss: 0.7626, Val Loss: 0.7958\n",
      "Saved best model for fold 5\n",
      "Epoch 2, Train Loss: 1.8244, Val Loss: 1.7779\n",
      "Epoch 3, Train Loss: 2.4281, Val Loss: 1.4837\n",
      "Epoch 4, Train Loss: 0.9750, Val Loss: 1.1392\n",
      "Epoch 5, Train Loss: 2.6797, Val Loss: 0.9980\n",
      "Fold 5 Metrics:\n",
      "MSE: 0.2589, MAE: 0.4667, R²: -1.0457\n",
      "\n",
      "Average best validation loss across all folds: 1.1273\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, brier_score_loss\n",
    "import torch\n",
    "\n",
    "dataset = FITSDataset(LABELS_CSV, DATA_DIR)\n",
    "kf = KFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "best_val_losses = []  # store best validation loss for each fold\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "    print(f\"\\nFold {fold+1}/{K_FOLDS}\")\n",
    "\n",
    "    train_subset = Subset(dataset, train_idx)\n",
    "    val_subset = Subset(dataset, val_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # Initialize model\n",
    "    model = FlexibleCNN(in_channels=IN_CHANNELS, n_layers=N_LAYERS, conv_channels=CONV_CHANNELS, \n",
    "                        kernel_size=KERNEL_SIZE, img_size=IMG_SIZE, dropout=DROPOUT, batch_norm=BATCH_NORM)\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = model.configure_optimizers(learning_rate=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        # Training\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "        train_loss = running_loss / len(train_subset)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss_total = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss_total += loss.item() * images.size(0)\n",
    "\n",
    "                all_preds.extend(outputs.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        val_loss = val_loss_total / len(val_subset)\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Save best model for this fold\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), f\"best_model_fold{fold+1}.pt\")\n",
    "            print(f\"Saved best model for fold {fold+1}\")\n",
    "\n",
    "    best_val_losses.append(best_val_loss)\n",
    "\n",
    "    # Compute continuous metrics for this fold\n",
    "    mse = mean_squared_error(all_labels, all_preds)\n",
    "    mae = mean_absolute_error(all_labels, all_preds)\n",
    "    r2 = r2_score(all_labels, all_preds)\n",
    "    # brier = brier_score_loss(all_labels, all_preds)\n",
    "\n",
    "    print(f\"Fold {fold+1} Metrics:\")\n",
    "    print(f\"MSE: {mse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n",
    "\n",
    "# Average validation loss across all folds\n",
    "avg_val_loss = sum(best_val_losses) / len(best_val_losses)\n",
    "print(f\"\\nAverage best validation loss across all folds: {avg_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # After cross-validation, train final model on all data\n",
    "# full_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# final_model = FlexibleCNN(in_channels=IN_CHANNELS, n_layers=N_LAYERS, conv_channels=CONV_CHANNELS,\n",
    "#                           kernel_size=KERNEL_SIZE, img_size=IMG_SIZE, dropout=DROPOUT, batch_norm=BATCH_NORM)\n",
    "# final_model = final_model.to(DEVICE)\n",
    "\n",
    "# criterion = nn.BCELoss()\n",
    "# optimizer = final_model.configure_optimizers(learning_rate=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# for epoch in range(EPOCHS):\n",
    "#     final_model.train()\n",
    "#     running_loss = 0\n",
    "#     for images, labels in full_loader:\n",
    "#         images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = final_model(images)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         running_loss += loss.item() * images.size(0)\n",
    "#     print(f\"Final Model Epoch {epoch+1}, Loss: {running_loss/len(dataset):.4f}\")\n",
    "\n",
    "# # Save final model\n",
    "# torch.save(final_model.state_dict(), \"final_model.pt\")\n",
    "# print(\"Final model saved to final_model.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnn_gc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
